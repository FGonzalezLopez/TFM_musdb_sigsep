{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "040cf021-cccc-4cb6-920c-4f27e31d5966",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import musdb\n",
    "import IPython.display as ipd\n",
    "import openunmix as opmux\n",
    "import torch\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import musdb\n",
    "import museval\n",
    "\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390371cf-d1f7-4ace-b24d-43194bb6bd73",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40f2360f-a78d-4f9b-b177-7000ebf6fd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "musdb_path = \"/home/paco/TFM/data/MUSDB18/\"\n",
    "data_path = \"/home/paco/TFM/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf233c05-3c15-4fbf-9dbe-4606956fde82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe73804a110>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18828b83-4893-4624-aa4e-9182968cad54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Hacemos inferencia en cpu, no consigo gestionar bien la memoria en gpu y estoy ahorrando un total de 5 minutos de procesamiento que va a ser una sola vez\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b323eae-0846-468d-920c-ef34cc584e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  GeForce RTX 3060 Laptop GPU\n",
      "GPU available: True\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", torch.cuda.get_device_name())\n",
    "print(\"GPU available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "391a5b87-feeb-4ee8-b243-a60551daa936",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "subtrack_length = 3\n",
    "prediction_overlap = 0.25\n",
    "source = 'source'\n",
    "targets = ['vocals', 'drums', 'bass', 'other']\n",
    "target = targets[0]\n",
    "# 8192 de training\n",
    "train_samples = 2**13\n",
    "# Todas las muestras de validación (1227)\n",
    "val_samples = 2**11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3993166-723d-4e30-8eaf-56c805e70f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dset =  musdb.DB(root=musdb_path, subsets=['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac2eec8-3617-431c-a8e5-6bddd8c3f0d9",
   "metadata": {},
   "source": [
    "# Definimos la arquitectura de la red..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35884e5b-829f-4a5c-b24d-f10a31136076",
   "metadata": {},
   "source": [
    "### Primero, la transformada de Fourier para codificar/decodificar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98d2ae5b-9e95-4f34-ae8c-d402cab0bb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sftf_window_size = 4096\n",
    "stft_window_hop = 1024\n",
    "stft_center = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8c02794-9f46-4bf2-b77d-ac92af8f60b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecSepSTFT_3(torch.nn.Module): # Buenos resultados con lr inicial = 1e-2\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Usamos la norma del complejo como hacen en OpenUnmix\n",
    "        self.complex_norm = opmux.transforms.ComplexNorm()\n",
    "        \n",
    "        self.encoder_stft = opmux.transforms.TorchSTFT(n_fft=sftf_window_size, n_hop=stft_window_hop, center=stft_center)\n",
    "        self.decoder_stft = opmux.transforms.TorchISTFT(n_fft=sftf_window_size, n_hop=stft_window_hop, center=stft_center)\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(((int(sftf_window_size/2)+1)*2), 256)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(256)\n",
    "        \n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(input_size=256,\n",
    "                                  hidden_size=int(256/2),\n",
    "                                  num_layers=3, \n",
    "                                  batch_first=True, \n",
    "                                  dropout=0, \n",
    "                                  bidirectional=True,\n",
    "                                  proj_size=0)\n",
    "        #self.fc_lstm_res = torch.nn.Linear(256*2,1024)\n",
    "        self.dropout = torch.nn.Dropout(0)\n",
    "        \n",
    "        self.fc2 = torch.nn.Linear(256, 256)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.fc3 = torch.nn.Linear(256,256)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.fc4 = torch.nn.Linear(256,(int(sftf_window_size/2)+1)*2)\n",
    "        self.bn4 = torch.nn.BatchNorm1d((int(sftf_window_size/2)+1)*2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pre_mix = x\n",
    "        # Calculamos la norma compleja (pasamos a dominio real)\n",
    "        x = self.complex_norm(x)\n",
    "        \n",
    "        # Tenemos una entrada en formato: (batch, canal, feature/banda, secuencia)\n",
    "        b_size, n_channel, n_feat, seq_len = x.size()\n",
    "\n",
    "        # Vamos a hacer un primer paso de codificación, para ello tenemos que dejar los datos en forma (batch, secuencia, features)\n",
    "        # Permutamos los datos a formato (batch, secuencia, canal, banda)\n",
    "        x = torch.permute(x, (0,3,1,2))\n",
    "        # Pasamos las dos últimas dimensiones (ahora son canal, banda) a una única (\"desenrollamos\" las features de cada canal en uno solo)\n",
    "        x = torch.reshape(x, (b_size, seq_len, n_channel * n_feat))\n",
    "        # Ponemos una capa fully connected, batch norm, y activación\n",
    "        x = self.fc1(x)\n",
    "        # Para el batch norm hay que tener el tensor en formato (batch, features, sequence)\n",
    "        x = torch.permute(x, (0,2,1))\n",
    "        # Batch norm\n",
    "        x = self.bn1(x)\n",
    "        # Ahora tenemos los datos en formato  (batch, features, sequence)\n",
    "        # La lstm los necesita en formato (batch, sequence, features)\n",
    "        x = torch.permute(x, (0,2,1))\n",
    "        # Activación\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x_skip_1 = x\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Nos quedamos con los estados de cada step de la secuencia\n",
    "        x_skip_lstm = x\n",
    "        x=self.lstm(x)[0]\n",
    "        x = x_skip_lstm + x\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        \n",
    "        # Ahora tenemos los datos en formato (batch, sequence, features)\n",
    "        # Aplicamos fc, bn, activation de nuevo\n",
    "        x = self.fc2(x)\n",
    "        x = torch.permute(x, (0,2,1))\n",
    "        x = self.bn2(x)\n",
    "        x = torch.permute(x, (0,2,1))\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x_skip_2 = x\n",
    "        \n",
    "        # Aplicamos una capa fc más para conseguir que features tenga un tamaño compatible con (canales, bandas_fourier_iniciales)\n",
    "        x = self.fc3(x)\n",
    "        x = torch.permute(x, (0,2,1))\n",
    "        x = self.bn3(x)\n",
    "        x = torch.permute(x, (0,2,1))\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        \n",
    "        x = x + x_skip_1\n",
    "        x = x + x_skip_2\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        x = torch.permute(x, (0,2,1))\n",
    "        x = self.bn4(x)\n",
    "        x = torch.permute(x, (0,2,1))\n",
    "        \n",
    "        # Redistribuimos y giramos para dejar los datos en formato: (batch, canal, feature/banda, secuencia, complejo)\n",
    "        x = torch.reshape(x, (b_size, seq_len, n_channel, n_feat, 1))\n",
    "        x = torch.permute(x, (0,2,3,1,4))\n",
    "        \n",
    "        # Aplicamos una sigmoidal para obtener una soft mask, y la aplicamos a la entrada\n",
    "        # x = torch.sigmoid(x)\n",
    "        \n",
    "        # Aplicamos x como una máscara a la stft de la entrada sin procesar\n",
    "        x = pre_mix * x\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab323fdc-449e-4130-8bb7-0fd2be78105f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecSepSTFT_3(\n",
       "  (complex_norm): ComplexNorm()\n",
       "  (encoder_stft): TorchSTFT()\n",
       "  (decoder_stft): TorchISTFT()\n",
       "  (fc1): Linear(in_features=4098, out_features=256, bias=True)\n",
       "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lstm): LSTM(256, 128, num_layers=3, batch_first=True, bidirectional=True)\n",
       "  (dropout): Dropout(p=0, inplace=False)\n",
       "  (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc3): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc4): Linear(in_features=256, out_features=4098, bias=True)\n",
       "  (bn4): BatchNorm1d(4098, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargamos el modelo y lo ponemos en modo inferencia\n",
    "model = torch.load(data_path+('model_%s.pt'%(target))).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f3a8e59-fb82-4d37-adca-e65cde23d11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "def test_model(in_model, in_data, in_target, in_seq_length = 3 * 44100, overlap_step=1, pred_batch_size = 128):\n",
    "    # Aseguramos que estamos en modo de evaluación\n",
    "    in_model = in_model.eval()\n",
    "\n",
    "    pred_tracks = []\n",
    "    # Para cada track en data...\n",
    "    for track in tqdm(in_data, desc='Estimando %s' % in_target):\n",
    "        # Extraemos el audio de la mezcla\n",
    "        full_X = track.audio.T\n",
    "        # Extraemos el audio del target\n",
    "        true_y = track.targets[in_target].audio.T\n",
    "        \n",
    "        x = []\n",
    "        x_indices = []\n",
    "\n",
    "        # Extraemos las muestras a predecir (la primera dimensión es el canal*)\n",
    "        for idx in np.arange(0,full_X.shape[1], int(overlap_step * in_seq_length)):\n",
    "            subtrack_padded = np.zeros((full_X.shape[0], in_seq_length))\n",
    "            subtrack = full_X[:, idx:(idx+in_seq_length)]\n",
    "            subtrack_padded[:, :subtrack.shape[1]] = subtrack\n",
    "            x.append(subtrack_padded)\n",
    "            x_indices.append(idx)\n",
    "        \n",
    "        # Pasamos las muestras por el modelo para obtener la predicción\n",
    "        # Creamos el tensor de salida\n",
    "        pred_y = torch.zeros(true_y.shape, dtype=torch.float32)\n",
    "        pred_y_samples = torch.zeros_like(pred_y)\n",
    "        \n",
    "        # Pasamos las muestras a tensor de Torch\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "        # Iteramos sobre los batch, realizando la predicción en cada uno\n",
    "        for idx in np.arange(0,x.shape[0], batch_size):\n",
    "            # Sacamos el batch\n",
    "            batch_pred = x[idx:idx+batch_size]\n",
    "            batch_indices = x_indices[idx:idx+batch_size]\n",
    "            \n",
    "            # Lo copiamos a device, codificamos con STFT\n",
    "            batch_pred = batch_pred.to(device)\n",
    "            batch_pred = model.encoder_stft(batch_pred)\n",
    "            # Predecimos\n",
    "            batch_pred = model(batch_pred)\n",
    "            # Decodificamos con stft\n",
    "            batch_pred = model.decoder_stft(batch_pred, length=in_seq_length)\n",
    "            # Lo copiamos a cpu de nuevo\n",
    "            batch_pred = batch_pred.detach().cpu()\n",
    "            \n",
    "            # Añadimos sobre el array completo de predicción\n",
    "            for ii in np.arange(batch_pred.shape[0]):\n",
    "                idx_start = batch_indices[ii]\n",
    "                idx_end = idx_start+batch_pred.shape[-1]\n",
    "                # Lo limitamos al final del array\n",
    "                idx_end = idx_end if (idx_end < pred_y.shape[1]) else pred_y.shape[1]\n",
    "                    \n",
    "                pred_partial = batch_pred[ii]\n",
    "                pred_y[:, idx_start:(idx_start+pred_partial.shape[1])] += pred_partial[:,0:(idx_end-idx_start)]\n",
    "                pred_y_samples[:, idx_start:(idx_start+pred_partial.shape[-1])] += 1\n",
    "        torch.cuda.empty_cache()\n",
    "        pred_y_samples[pred_y_samples == 0] = 1\n",
    "        pred_y = pred_y / pred_y_samples\n",
    "        \n",
    "        pred_tracks.append(pred_y.detach())\n",
    "        \n",
    "    return pred_tracks\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d96540c-d96c-4591-bc09-1c81ed27220b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# pred_samples = test_model(model, test_dset, target, subtrack_length * test_dset[0].rate, overlap_step=prediction_overlap, pred_batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c8f73557-c3c7-4c52-961a-b7aeef3f0406",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1a6f07fd-ddd3-4075-a96d-bef87898abef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = test_dset[sample_idx].audio.T\n",
    "# sample = sample[sample_idx].cpu().detach()\n",
    "# ipd.Audio(sample, rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f6922a13-e0d1-4748-b5ec-cccf7a0bf28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = estimates['vocals'][sample_idx]\n",
    "# sample = sample[sample_idx].cpu().detach()\n",
    "# ipd.Audio(sample.numpy(), rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ac7d39-58df-4482-8a51-14533e2d26f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67a28484-9dba-481c-993e-7d3b306cc526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b9e430de9447229e56c16b90bc3487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dccf4952f134c18a1c3e7c9ff1124c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Estimando vocals:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f5bd2ba61646e0bc9fe3cc2f4c4be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Estimando drums:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "950804848f504988b0583ed5ef059153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Estimando bass:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2146892895042ae9434b55a1f5e12f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Estimando other:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ahora lo hacemos para todos los targets\n",
    "estimates = {}\n",
    "\n",
    "for target in tqdm(targets):\n",
    "    # Cargamos el modelo y lo ponemos en modo inferencia\n",
    "    model = torch.load(data_path+('model_%s.pt'%(target))).to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Sacamos las estimaciones de target\n",
    "    target_preds = test_model(model, test_dset, target, subtrack_length * test_dset[0].rate, overlap_step=prediction_overlap, pred_batch_size=batch_size)\n",
    "    \n",
    "    # Las guardamos en estimates\n",
    "    estimates[target] = target_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5537af-586d-4679-a8ee-ea995d809ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "03dec072-f451-4f88-bd16-768c3dcff69e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d3ea6cfd7d84251948474313a3400be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for idx in tqdm(np.arange(len(test_dset))):\n",
    "    test_dset.save_estimates(user_estimates={target:(estimates.get(target)[idx].numpy().T) for target in targets}, track=test_dset[idx], estimates_dir = data_path+'MUSDB18_estimates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59846acb-df02-43f5-aea2-01a5aebef741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7361efb-e52b-4170-9a5b-ff61a8c1d326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ea391cbcf3426592149ec25f77fe5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 12min 55s, sys: 13min 31s, total: 1h 26min 27s\n",
      "Wall time: 1h 24min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = museval.EvalStore(frames_agg='median', tracks_agg='median')\n",
    "\n",
    "for idx, track in tqdm(enumerate(test_dset), total=len(test_dset)):\n",
    "    track_estimates = {target:(estimates.get(target)[idx].numpy().T) for target in targets}\n",
    "    results.add_track(museval.eval_mus_track(track, track_estimates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e104c1e-2bf4-4f3d-aa2c-3ac3158ba15f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Aggrated Scores (median over frames, median over tracks)\n",
       "vocals          ==> SDR:   4.172  SIR:   6.709  ISR:   9.054  SAR:   6.139  \n",
       "drums           ==> SDR:   3.746  SIR:   6.284  ISR:   6.524  SAR:   5.314  \n",
       "bass            ==> SDR:   3.400  SIR:   4.411  ISR:   7.096  SAR:   5.205  \n",
       "other           ==> SDR:   2.560  SIR:   2.768  ISR:   4.989  SAR:   4.495  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6ab808e6-b218-4b4a-9b78-6f98e2f151f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons = museval.MethodStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "27338586-aa2e-4bdd-9cad-453d99b9a87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons.add_evalstore(results, name=\"RRSEP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8661be-bd74-461e-a22e-7d963df55916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparisons.save(data_path+'compared_results_dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fe266980-7028-4b15-be9f-eba8d0353ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading SISEC18 Evaluation data...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "comparisons = museval.MethodStore()\n",
    "comparisons.add_sisec18()\n",
    "comparisons.add_evalstore(results, name=\"RRSEP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9ab47706-49fd-4c58-a24b-ba930f5a4f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "bass      4.875210\n",
       "drums     5.555230\n",
       "other     3.896905\n",
       "vocals    6.099680\n",
       "Name: score, dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_df[agg_df.method == 'RRSEP'].groupby('target')['score'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "00d385c9-9679-47f4-bef0-7b5e24521d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ISR', 'SAR', 'SDR', 'SIR'], dtype=object)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_df.metric.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bf911b9e-67c6-4d73-ba52-61ae00d1d225",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df = comparisons.agg_frames_scores().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "72c7c7e0-65e0-4d2d-953a-3fca7fc1f937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>track</th>\n",
       "      <th>target</th>\n",
       "      <th>metric</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2DFT</td>\n",
       "      <td>AM Contra - Heart Peripheral</td>\n",
       "      <td>accompaniment</td>\n",
       "      <td>ISR</td>\n",
       "      <td>12.456640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2DFT</td>\n",
       "      <td>AM Contra - Heart Peripheral</td>\n",
       "      <td>accompaniment</td>\n",
       "      <td>SAR</td>\n",
       "      <td>8.595240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2DFT</td>\n",
       "      <td>AM Contra - Heart Peripheral</td>\n",
       "      <td>accompaniment</td>\n",
       "      <td>SDR</td>\n",
       "      <td>8.582600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2DFT</td>\n",
       "      <td>AM Contra - Heart Peripheral</td>\n",
       "      <td>accompaniment</td>\n",
       "      <td>SIR</td>\n",
       "      <td>22.414620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2DFT</td>\n",
       "      <td>AM Contra - Heart Peripheral</td>\n",
       "      <td>vocals</td>\n",
       "      <td>ISR</td>\n",
       "      <td>17.195920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>WK</td>\n",
       "      <td>Zeno - Signs</td>\n",
       "      <td>other</td>\n",
       "      <td>SIR</td>\n",
       "      <td>6.536005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>WK</td>\n",
       "      <td>Zeno - Signs</td>\n",
       "      <td>vocals</td>\n",
       "      <td>ISR</td>\n",
       "      <td>8.265570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>WK</td>\n",
       "      <td>Zeno - Signs</td>\n",
       "      <td>vocals</td>\n",
       "      <td>SAR</td>\n",
       "      <td>4.273505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>WK</td>\n",
       "      <td>Zeno - Signs</td>\n",
       "      <td>vocals</td>\n",
       "      <td>SDR</td>\n",
       "      <td>4.499505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>WK</td>\n",
       "      <td>Zeno - Signs</td>\n",
       "      <td>vocals</td>\n",
       "      <td>SIR</td>\n",
       "      <td>10.509690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      method                         track         target metric      score\n",
       "0       2DFT  AM Contra - Heart Peripheral  accompaniment    ISR  12.456640\n",
       "1       2DFT  AM Contra - Heart Peripheral  accompaniment    SAR   8.595240\n",
       "2       2DFT  AM Contra - Heart Peripheral  accompaniment    SDR   8.582600\n",
       "3       2DFT  AM Contra - Heart Peripheral  accompaniment    SIR  22.414620\n",
       "4       2DFT  AM Contra - Heart Peripheral         vocals    ISR  17.195920\n",
       "...      ...                           ...            ...    ...        ...\n",
       "24995     WK                  Zeno - Signs          other    SIR   6.536005\n",
       "24996     WK                  Zeno - Signs         vocals    ISR   8.265570\n",
       "24997     WK                  Zeno - Signs         vocals    SAR   4.273505\n",
       "24998     WK                  Zeno - Signs         vocals    SDR   4.499505\n",
       "24999     WK                  Zeno - Signs         vocals    SIR  10.509690\n",
       "\n",
       "[25000 rows x 5 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5130e03b-b83a-4e09-b3af-5bebb7253d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paco/anaconda3/envs/ds/lib/python3.8/site-packages/seaborn/axisgrid.py:392: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, axes = plt.subplots(nrow, ncol, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "all_metrics = ['SDR', 'SIR', 'SAR', 'ISR']\n",
    "all_targets = ['vocals', 'drums', 'bass', 'other']\n",
    "\n",
    "for metric in all_metrics:\n",
    "    for target in all_targets:\n",
    "        metrics = [metric,]\n",
    "        selected_targets = [target,]\n",
    "\n",
    "        agg_df = comparisons.agg_frames_scores().reset_index()\n",
    "\n",
    "        sns.set()\n",
    "        sns.set_context(\"notebook\")\n",
    "\n",
    "\n",
    "        oracles = [\n",
    "            'IBM1', 'IBM2', 'IRM1', 'IRM2', 'MWF', 'IMSK'\n",
    "        ]\n",
    "\n",
    "        # Convert to Pandas Dataframes\n",
    "        agg_df['oracle'] = agg_df.method.isin(oracles)\n",
    "        agg_df = agg_df[agg_df.target.isin(selected_targets)].dropna()\n",
    "\n",
    "        # Get sorting keys (sorted by median of SDR:vocals)\n",
    "        df_sort_by = agg_df[\n",
    "            (agg_df.metric == metrics[0]) &\n",
    "            (agg_df.target == selected_targets[0])\n",
    "        ]\n",
    "\n",
    "        methods_by_sdr = df_sort_by.score.groupby(\n",
    "            df_sort_by.method\n",
    "        ).median().sort_values().index.tolist()\n",
    "\n",
    "        # df = df[df.target == \"vocals\"]\n",
    "        g = sns.FacetGrid(\n",
    "            agg_df,\n",
    "            row=\"target\",\n",
    "            col=\"metric\",\n",
    "            row_order=selected_targets,\n",
    "            col_order=metrics,\n",
    "            height=6,\n",
    "            sharex=False,\n",
    "            aspect=3\n",
    "        )\n",
    "        g = (g.map(\n",
    "            sns.boxplot,\n",
    "            \"score\",\n",
    "            \"method\",\n",
    "            \"oracle\",\n",
    "            orient='h',\n",
    "            order=methods_by_sdr[::-1],\n",
    "            hue_order=[True, False],\n",
    "            showfliers=False,\n",
    "            notch=True\n",
    "        ))\n",
    "\n",
    "        g.ax.artists[np.where(np.array(methods_by_sdr[::-1]) == 'RRSEP')[0][0]].set_facecolor('orange')\n",
    "\n",
    "        g.fig.tight_layout()\n",
    "        plt.subplots_adjust(hspace=0.2, wspace=0.1)\n",
    "        g.fig.savefig(\n",
    "            data_path+\"plots/boxplot_%s_%s.png\" % (metrics[0],selected_targets[0]),\n",
    "            bbox_inches='tight',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c03fc05-1f31-43f1-b840-a2ac86f88fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b7c64c-5f5c-4ee3-a2d5-90a794df720d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
